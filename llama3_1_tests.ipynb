{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b0095-ffac-4e1b-ba07-2176b3ce0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142627b-d976-46a8-b47d-fe23fa9b4091",
   "metadata": {},
   "source": [
    "#### Define the input texts for the model to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294a91e2-7ec1-45cc-b255-d939723b3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"How does quantum computing work?\",\n",
    "    \"What are the benefits of machine learning?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9423a6-1fbe-465b-9f06-bb1a4a60aeca",
   "metadata": {},
   "source": [
    "### Load the **Llama 3.1 8B** model and tokenizer, then perform inference on the input texts.\n",
    "- This function requires at least 32GiB of GPU memory to run efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f894d7c-7d88-4675-a244-e2fb7f4cc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_8B(input_texts):\n",
    "    # Load model directly\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "    \n",
    "    ### NOTE: THIS MODEL REQUIRES AT LEAST 32GiB OF GPU MEMORY ###\n",
    "    # Move the model to the GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=200)\n",
    "        generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Question: {input_texts[i]}\")\n",
    "        print(f\"Answer: {text}\")\n",
    "        print()\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del model\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736a4d9-0186-4761-9160-426a72480023",
   "metadata": {},
   "source": [
    "### Perform inference on the defined input texts using the **8B** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77bdd51-3f79-4381-89e1-aa2ebf410303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:37<00:00,  9.32s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Explain the theory of relativity.\n",
      "Answer: Explain the theory of relativity. What are the main points of Einstein's theory of relativity?\n",
      "Einstein's theory of relativity is one of the most important scientific ideas of the 20th century. It has had a huge impact on our understanding of the universe, and has led to many technological advances.\n",
      "The theory of relativity is based on the idea that the laws of physics are the same for all observers, regardless of their speed or location. This means that the laws of physics are the same for all observers, regardless of their speed or location. This is a fundamental principle of the theory of relativity.\n",
      "The theory of relativity also says that the speed of light is the same for all observers, regardless of their speed or location. This is a fundamental principle of the theory of relativity.\n",
      "The theory of relativity also says that time is relative. This means that time is not absolute, but is relative to the observer. This is a fundamental principle of the theory\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Answer: What is the capital of France?_REFLECT_\n",
      "What is the capital of France? - A. New York\n",
      "B. Paris\n",
      "C. Washington\n",
      "D. Madrid\n",
      "Answer: B\n",
      "Explanation:  The capital of France is Paris.\n",
      "\n",
      "Question: How does quantum computing work?\n",
      "Answer: How does quantum computing work?:// Theoretical computer science: Quantum computing is a type of computation that uses quantum-mechanical phenomena, such as superposition and entanglement, to represent and structure information. A quantum computer is used to perform factoring, and is the basis of the factoring problem. A quantum computer employs a different principle than a classical computer. Classical computers use bits to represent and store information, while quantum computers use quantum bits, or qubits. Qubits can be in more than one state at a time, which makes them more efficient for some problems than classical computers. There are two types of quantum computers: general-purpose quantum computers and adiabatic quantum computers. General-purpose quantum computers are more powerful than classical computers. Adiabatic quantum computers are less powerful than classical computers. Quantum computers are still in their infancy, but they have the potential to be used in many different areas, such as factoring, quantum cryptography, and quantum teleport\n",
      "\n",
      "Question: What are the benefits of machine learning?\n",
      "Answer: What are the benefits of machine learning? What are the benefits of machine learning?\n",
      "What are the benefits of machine learning?\n",
      "Machine learning is a field of computer science that uses statistical techniques to give computers the ability to learn without being explicitly programmed. Machine learning is used in a wide range of applications, from email filtering to stock market predictions. In this article, we will discuss the benefits of machine learning.\n",
      "Machine learning is a field of artificial intelligence that deals with the creation of algorithms that can learn from data. These algorithms are able to improve their performance over time without being explicitly programmed. Machine learning is used in a variety of applications, from email filtering to stock market predictions.\n",
      "What are the benefits of machine learning?\n",
      "There are many benefits of machine learning. Some of the most notable benefits include:\n",
      "1. Improved accuracy: Machine learning algorithms can learn from data and improve their performance over time. This means that they can be used to improve the accuracy of predictions.\n",
      "2. Increased efficiency: Machine learning algorithms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_8B(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92ce07-a9ab-4122-8cd3-cd975affc430",
   "metadata": {},
   "source": [
    "### Load the **Llama 3.1 8B-Instruct** model and tokenizer, then perform inference on the input texts.\n",
    "- This function requires at least 32GiB of GPU memory to run efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b080091f-3ca5-4971-8c3c-a8ef5ad24aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_8B_instruct(input_texts):\n",
    "\n",
    "    # Load model directly\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "    \n",
    "    ### NOTE: THIS MODEL REQUIRES AT LEAST 32GiB OF GPU MEMORY ###\n",
    "    # Move the model to the GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=200)\n",
    "        generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Question: {input_texts[i]}\")\n",
    "        print(f\"Answer: {text}\")\n",
    "        print()\n",
    "        \n",
    "    # Clear GPU memory\n",
    "    del model\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb9d59-cd09-4896-abc3-835fdf486314",
   "metadata": {},
   "source": [
    "### Perform inference on the defined input texts using the **8B** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2761c26-ac0b-47c2-918e-095ca061540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.34s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Explain the theory of relativity.\n",
      "Answer: Explain the theory of relativity. Albert Einstein's theory of relativity, which was introduced in 1905 and 1915, revolutionized our understanding of space and time. The theory has two main components: special relativity and general relativity.\n",
      "A. Special Relativity\n",
      "Special relativity, which was introduced in 1905, posits that the laws of physics are the same for all observers in uniform motion relative to one another. This theory challenged the long-held notion of absolute time and space. Key concepts in special relativity include:\n",
      "1. Time dilation: Time appears to pass more slowly for an observer in motion relative to a stationary observer.\n",
      "2. Length contraction: Objects appear shorter to an observer in motion relative to a stationary observer.\n",
      "3. Relativity of simultaneity: Two events that are simultaneous for one observer may not be simultaneous for another observer in a different state of motion.\n",
      "4. Equivalence of mass and energy: Mass (m) and energy\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Answer: What is the capital of France?\n",
      "A. Paris\n",
      "B. Lyon\n",
      "C. Marseille\n",
      "D. Bordeaux\n",
      "\n",
      "Answer: A\n",
      "Reasoning Skill: This question requires the ability to recall a basic fact about a country, which is a fundamental aspect of geography. To answer this question correctly, one must be able to access and retrieve information from memory, which is a key component of the reasoning skill for Recall. \n",
      "\n",
      "Note: The other options (Lyon, Marseille, and Bordeaux) are all major cities in France, but they are not the capital. This makes them plausible distractors, but incorrect answers. \n",
      "\n",
      "This type of question is relevant to the topic of Geography and requires the student to demonstrate their knowledge of basic geographical facts. It is a simple question that requires a recall of information, making it a good example of a question that tests the reasoning skill of Recall. \n",
      "\n",
      "In a real-world scenario, being able to recall the capital of a country can be useful in a\n",
      "\n",
      "Question: How does quantum computing work?\n",
      "Answer: How does quantum computing work? the basics\n",
      "Quantum computing is a new paradigm for computing that uses the principles of quantum mechanics to perform calculations. It's a bit like a super-powerful calculator that can solve certain problems much faster than a classical computer.\n",
      "Here's a simplified explanation of how quantum computing works:\n",
      "**Classical computers vs. Quantum computers**\n",
      "Classical computers use \"bits\" to store and process information. A bit can be either 0 or 1, and it's like a light switch that's either on or off. Quantum computers, on the other hand, use \"qubits\" (quantum bits). Qubits can exist in multiple states at the same time, which is known as a superposition.\n",
      "**Superposition**\n",
      "Imagine you have a coin that can be either heads or tails. A classical computer would represent this as a single bit, either 0 (heads) or 1 (tails). A qubit, however, can represent both heads\n",
      "\n",
      "Question: What are the benefits of machine learning?\n",
      "Answer: What are the benefits of machine learning? Machine learning can be used in a variety of ways, including image and speech recognition, natural language processing, and predictive analytics. It can be used to improve the accuracy and efficiency of business processes, and to gain insights and make predictions about customer behavior.\n",
      "Machine learning can be used in a variety of ways, including:\n",
      "Image and speech recognition: Machine learning algorithms can be trained to recognize patterns in images and speech, allowing for applications such as facial recognition, object detection, and voice assistants.\n",
      "Natural language processing: Machine learning algorithms can be trained to understand and generate human language, allowing for applications such as chatbots, language translation, and text summarization.\n",
      "Predictive analytics: Machine learning algorithms can be trained to make predictions about future events based on historical data, allowing for applications such as demand forecasting, risk assessment, and customer segmentation.\n",
      "Machine learning can be used to improve the accuracy and efficiency of business processes, and to gain insights and make predictions about customer behavior\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_8B_instruct(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2489b3-65a2-42df-929b-c51d11628799",
   "metadata": {},
   "source": [
    "### Load the **Llama 3.1 70B** model and tokenizer, then perform inference on the input texts.\n",
    "- This function requires at least 80GiB of GPU memory to run efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d541f1-966f-4677-8c1c-e3edc0e0d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_70B(input_texts):\n",
    "    # Load model directly\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-70B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-70B\")\n",
    "\n",
    "    ### NOTE: THIS MODEL REQUIRES AT LEAST 80GiB OF GPU MEMORY ###\n",
    "    # Move the model to the GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=200)\n",
    "        generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Question: {input_texts[i]}\")\n",
    "        print(f\"Answer: {text}\")\n",
    "        print()\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del model\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c882a2a-ccaa-45d0-a489-ba8b9ea02ad1",
   "metadata": {},
   "source": [
    "### Perform inference on the defined input texts using the **70B** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2ac4f4-a397-47b8-9cd3-e65a69e20c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 30/30 [05:41<00:00, 11.39s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 414.25 MiB is free. Including non-PyTorch memory, this process has 43.93 GiB memory in use. Of the allocated memory 43.61 GiB is allocated by PyTorch, and 13.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minference_70B\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36minference_70B\u001b[0;34m(input_texts)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m### NOTE: THIS MODEL REQUIRES AT LEAST 80GiB OF GPU MEMORY ###\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Move the model to the GPU\u001b[39;00m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/transformers/modeling_utils.py:2861\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2858\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2859\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2860\u001b[0m         )\n\u001b[0;32m-> 2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 414.25 MiB is free. Including non-PyTorch memory, this process has 43.93 GiB memory in use. Of the allocated memory 43.61 GiB is allocated by PyTorch, and 13.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "inference_70B(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e4c2c-d6d3-44f8-ba18-dda72dc939f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-py311",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
