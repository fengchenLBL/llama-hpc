{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f4b0095-ffac-4e1b-ba07-2176b3ce0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142627b-d976-46a8-b47d-fe23fa9b4091",
   "metadata": {},
   "source": [
    "#### Define the input texts for the model to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294a91e2-7ec1-45cc-b255-d939723b3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"How does quantum computing work?\",\n",
    "    \"What are the benefits of machine learning?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9423a6-1fbe-465b-9f06-bb1a4a60aeca",
   "metadata": {},
   "source": [
    "### Load the **Llama 3.1 8B** model and tokenizer, then perform inference on the input texts.\n",
    "- This function requires at least 32GiB of GPU memory to run efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f894d7c-7d88-4675-a244-e2fb7f4cc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_8B(input_texts):\n",
    "    # Load model directly\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "    \n",
    "    ### NOTE: THIS MODEL REQUIRES AT LEAST 32GiB OF GPU MEMORY ###\n",
    "    # Move the model to the GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "    model.to(device)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=200)\n",
    "        generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Question: {input_texts[i]}\")\n",
    "        print(f\"Answer: {text}\")\n",
    "        print()\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del model\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736a4d9-0186-4761-9160-426a72480023",
   "metadata": {},
   "source": [
    "### Perform inference on the defined input texts using the **8B** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77bdd51-3f79-4381-89e1-aa2ebf410303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  9.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Explain the theory of relativity.\n",
      "Answer: Explain the theory of relativity. What are the 4 postulates of special relativity? 2019-01-19\n",
      "What is the theory of relativity?\n",
      "If a person is moving with the same velocity as a photon, they would see the photon moving at the speed of light. It is also called the theory of relativity. It is the theory of relativity that explains how the laws of physics are the same for all non-accelerating observers, and are independent of the state of motion of the observers. The theory of relativity is a theory that states that the laws of physics are the same for all observers that are moving at a constant velocity, regardless of their relative motion. In 1905, Einstein published his paper on the Special Theory of Relativity. The theory of relativity was developed by Albert Einstein in 1905. The theory of relativity is a theory of gravitation in which gravity is treated as a geometric property of space and\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Answer: What is the capital of France?://www.census.gov/geo/maps-data/data/tiger.html. The data are from the U.S. Census Bureau's 2010 TIGER/Line Shapefiles. The data are from the U.S. Census Bureau's 2010 TIGER/Line Shapefiles. What is the capital of North Carolina? This data product is the primary source of geographic information for the United States, Puerto Rico and the Island Areas. The data are from the U.S. Census Bureau's 2010 TIGER/Line Shapefiles. What is the capital of New Mexico? The data are from the U.S. Census Bureau's 2010 TIGER/Line Shapefiles. The data are from the U.S. Census Bureau's 2010 TIGER/Line Shapefiles. What is the capital of New Jersey? The data are from the U.S. Census Bureau's 2010 TIGER/Line Shape\n",
      "\n",
      "Question: How does quantum computing work?\n",
      "Answer: How does quantum computing work?://\n",
      "A quantum computer is a machine that can perform computations that are exponentially faster than conventional computers. It uses quantum mechanics to perform calculations on data, as opposed to classical mechanics. The difference is that quantum mechanics is based on the laws of quantum mechanics, and classical mechanics is based on the laws of classical mechanics. Quantum mechanics is the theory that explains how the world works at the atomic and subatomic levels, and classical mechanics is the theory that explains how the world works at the macroscopic level. The first quantum computer was built in 2001 by a team of researchers at IBM, and the first practical quantum computer was built in 2016 by a team of researchers at Google. The most recent quantum computer was built in 2020 by a team of researchers at IBM, and it is the first computer to use quantum mechanics to perform calculations. Quantum computing is a rapidly growing field of computer science and is used to perform a wide variety of\n",
      "\n",
      "Question: What are the benefits of machine learning?\n",
      "Answer: What are the benefits of machine learning? How can it help your business? How can you get started? In this post, we’ll cover everything you need to know about machine learning, from the basics to advanced concepts. We’ll also provide a step-by-step guide on how to get started with machine learning, so you can start reaping the benefits of this powerful technology.\n",
      "Machine learning is a branch of artificial intelligence that deals with the design and development of algorithms that can learn from and make predictions on data. Machine learning is a subset of artificial intelligence, and it is a field that is rapidly growing in popularity. Machine learning algorithms are used in a variety of applications, including data mining, pattern recognition, and prediction.\n",
      "There are two main types of machine learning: supervised and unsupervised. Supervised machine learning algorithms are trained on labeled data, while unsupervised machine learning algorithms are trained on unlabeled data. There are also two main types of unsupervised machine learning: clustering and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_8B(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92ce07-a9ab-4122-8cd3-cd975affc430",
   "metadata": {},
   "source": [
    "### Load the **Llama 3.1 8B-Instruct** model and tokenizer, then perform inference on the input texts.\n",
    "- This function requires at least 32GiB of GPU memory to run efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b080091f-3ca5-4971-8c3c-a8ef5ad24aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_8B_instruct(input_texts):\n",
    "\n",
    "    # Load model directly\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "    \n",
    "    ### NOTE: THIS MODEL REQUIRES AT LEAST 32GiB OF GPU MEMORY ###\n",
    "    # Move the model to the GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "    model.to(device)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=200)\n",
    "        generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Question: {input_texts[i]}\")\n",
    "        print(f\"Answer: {text}\")\n",
    "        print()\n",
    "        \n",
    "    # Clear GPU memory\n",
    "    del model\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb9d59-cd09-4896-abc3-835fdf486314",
   "metadata": {},
   "source": [
    "### Perform inference on the defined input texts using the **8B** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2761c26-ac0b-47c2-918e-095ca061540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00, 11.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Explain the theory of relativity.\n",
      "Answer: Explain the theory of relativity. Relativity is a fundamental concept in physics that explains how space and time are intertwined and how they are affected by gravity and motion. There are two main components to the theory of relativity: special relativity and general relativity.\n",
      "Special relativity, developed by Albert Einstein in 1905, posits that the laws of physics are the same for all observers in uniform motion relative to one another. This means that time and space are relative, and their measurement depends on the observer's frame of reference. The theory also introduces the concept of time dilation, where time appears to pass slower for an observer in motion relative to a stationary observer. Additionally, special relativity explains the concept of length contraction, where objects appear shorter to an observer in motion relative to a stationary observer.\n",
      "General relativity, developed by Einstein in 1915, builds upon special relativity and introduces the concept of gravity as a curvature of spacetime caused by massive objects. According to\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Answer: What is the capital of France?\n",
      "A. Paris\n",
      "B. Lyon\n",
      "C. Berlin\n",
      "D. Rome\n",
      "Answer: A\n",
      "Reasoning Skill: Identifying Pros And Cons\n",
      "In this question, the correct answer is A. Paris. To arrive at this answer, one needs to identify the pros of Paris being the capital of France, such as its historical significance, cultural importance, and political centrality. On the other hand, the cons of the other options, such as Lyon being a major city but not the capital, Berlin being the capital of Germany, and Rome being the capital of Italy, need to be considered and eliminated. This requires analyzing the information, weighing the pros and cons, and making an informed decision. \n",
      "\n",
      "Note: This question is not directly related to the topic of the provided textbook, but it is a general knowledge question that requires the same type of reasoning skill. If you'd like, I can create a question that is more relevant to the topic\n",
      "\n",
      "Question: How does quantum computing work?\n",
      "Answer: How does quantum computing work? we explain it in simple terms\n",
      "Quantum computing is a type of computing that uses the principles of quantum mechanics to perform calculations and operations on data. It's a relatively new and rapidly evolving field, but we'll try to break it down in simple terms.\n",
      "Imagine you have a big box of different colored balls. Each ball represents a piece of information, like a 0 or a 1. In classical computing, each ball can only be one color at a time. But in quantum computing, each ball can exist in multiple colors simultaneously. This is called a \"superposition.\"\n",
      "Now, imagine you have a special machine that can look at all the balls in the box at the same time. This machine can perform calculations on all the balls simultaneously, which is much faster than looking at each ball one by one.\n",
      "Another key feature of quantum computing is \"entanglement.\" Imagine you have two balls, one red and one blue. In classical computing\n",
      "\n",
      "Question: What are the benefits of machine learning?\n",
      "Answer: What are the benefits of machine learning? \n",
      "Machine learning is a subset of artificial intelligence (AI) that enables machines to learn from data and improve their performance on a task over time without being explicitly programmed. The benefits of machine learning include:\n",
      "1. Improved accuracy: Machine learning algorithms can analyze large amounts of data and make predictions or decisions with a high degree of accuracy.\n",
      "2. Increased efficiency: Machine learning can automate tasks that would otherwise require human intervention, freeing up time for more strategic and creative work.\n",
      "3. Enhanced decision-making: Machine learning can analyze complex data sets and provide insights that would be difficult or impossible for humans to obtain.\n",
      "4. Personalization: Machine learning can be used to personalize experiences for individuals, such as recommending products or services based on their preferences and behavior.\n",
      "5. Predictive maintenance: Machine learning can be used to predict when equipment or systems are likely to fail, allowing for proactive maintenance and reducing downtime.\n",
      "6. Improved customer service: Machine learning can be used to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_8B_instruct(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2489b3-65a2-42df-929b-c51d11628799",
   "metadata": {},
   "source": [
    "### Load the **Llama 3.1 70B** model and tokenizer, then perform inference on the input texts.\n",
    "- This function requires at least 80GiB of GPU memory to run efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d541f1-966f-4677-8c1c-e3edc0e0d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_70B(input_texts):\n",
    "    # Load model directly\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-70B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-70B\")\n",
    "\n",
    "    ### NOTE: THIS MODEL REQUIRES AT LEAST 80GiB OF GPU MEMORY ###\n",
    "    # Move the model to the GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "    model.to(device)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=200)\n",
    "        generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Question: {input_texts[i]}\")\n",
    "        print(f\"Answer: {text}\")\n",
    "        print()\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del model\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c882a2a-ccaa-45d0-a489-ba8b9ea02ad1",
   "metadata": {},
   "source": [
    "### Perform inference on the defined input texts using the **70B** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2ac4f4-a397-47b8-9cd3-e65a69e20c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 30/30 [05:41<00:00, 11.39s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 414.25 MiB is free. Including non-PyTorch memory, this process has 43.93 GiB memory in use. Of the allocated memory 43.61 GiB is allocated by PyTorch, and 13.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minference_70B\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36minference_70B\u001b[0;34m(input_texts)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m### NOTE: THIS MODEL REQUIRES AT LEAST 80GiB OF GPU MEMORY ###\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Move the model to the GPU\u001b[39;00m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/transformers/modeling_utils.py:2861\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2858\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2859\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2860\u001b[0m         )\n\u001b[0;32m-> 2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/miniconda3/envs/llama-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 414.25 MiB is free. Including non-PyTorch memory, this process has 43.93 GiB memory in use. Of the allocated memory 43.61 GiB is allocated by PyTorch, and 13.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "inference_70B(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e4c2c-d6d3-44f8-ba18-dda72dc939f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-py311",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
